{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6361fb8a-5113-4057-93b3-a138c8b7517b",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation \n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**Retrieval Augmented Generation (RAG)** is an AI architecture that combines the power of large language models (LLMs) with external information retrieval systems (such as vector stores or search engines). The core idea is to augment the generative capabilities of LLMs with relevant, up-to-date, or domain-specific knowledge retrieved from a database, document store, or the web.\n",
    "\n",
    "## Why Use RAG?\n",
    "\n",
    "- **Improved Accuracy:** RAG allows the LLM to access facts and information not present in its training data, reducing hallucinations.\n",
    "- **Up-to-Date Answers:** Enables the model to answer questions about recent events or specialized topics.\n",
    "- **Domain Adaptation:** RAG can work with proprietary or private data, making it valuable for enterprises and custom applications.\n",
    "\n",
    "## How Does RAG Work?\n",
    "\n",
    "RAG typically involves two main stages:\n",
    "\n",
    "### 1. **Retrieval**\n",
    "- When a user submits a query, the system first retrieves relevant documents, passages, or data chunks from an external source (vector store, search index, or database).\n",
    "- Retrieval is often semantic (using embeddings) but can use keyword search or hybrid methods.\n",
    "\n",
    "### 2. **Generation**\n",
    "- The retrieved documents are provided as additional context to the LLM.\n",
    "- The LLM generates an answer by grounding its response in both the retrieved context and its own pre-trained knowledge.\n",
    "\n",
    "**Simple RAG Workflow:**\n",
    "1. **User Query:** \"What is Retrieval Augmented Generation?\"\n",
    "2. **Retriever:** Finds relevant documents or passages about RAG.\n",
    "3. **Generator (LLM):** Reads the retrieved context and crafts a grounded, informed response.\n",
    "\n",
    "\n",
    "## RAG Architecture Diagram\n",
    "\n",
    "```\n",
    "User Query\n",
    "     |\n",
    "     v\n",
    "+------------+         +-----------------+\n",
    "|  Retriever | ----->  |  Relevant Docs  |\n",
    "+------------+         +-----------------+\n",
    "     |                          |\n",
    "     v                          v\n",
    "+----------------------------------------+\n",
    "|      Large Language Model (LLM)        |\n",
    "|  (with retrieved docs as extra input)  |\n",
    "+----------------------------------------+\n",
    "     |\n",
    "     v\n",
    "Generated Answer\n",
    "```\n",
    "\n",
    "## Example: RAG in Practice\n",
    "\n",
    "1. **User asks:** \"What is LangChain?\"\n",
    "2. **Retriever** searches docs, Wikipedia, or a vector DB and finds relevant paragraphs about LangChain.\n",
    "3. **LLM** receives both the user query and the retrieved info.\n",
    "4. **LLM generates:** \"LangChain is a framework for developing applications powered by language models, enabling retrieval, orchestration, and more.\"\n",
    "\n",
    "\n",
    "## Benefits of RAG\n",
    "\n",
    "- **Reduced Hallucination:** Answers are grounded in real data.\n",
    "- **Custom Knowledge:** Leverage internal/company documents not in public LLM data.\n",
    "- **Scalability:** Update the knowledge base without retraining the model.\n",
    "\n",
    "**Summary:**  \n",
    "RAG (Retrieval Augmented Generation) is a technique that combines LLMs and external retrieval systems to provide more accurate, current, and reliable answers by grounding generation in factual context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cf414f-cda2-4004-88ba-a1fbd33363a5",
   "metadata": {},
   "source": [
    "# What is In-Context Learning?\n",
    "\n",
    "**In-Context Learning (ICL)** is a capability of modern large language models (LLMs) where the model learns to perform new tasks or follow patterns by observing a few examples provided directly in the input prompt, without any additional parameter tuning or gradient updates. The model uses the provided “context” (examples, instructions, or demonstrations) to infer the task and generate appropriate outputs.\n",
    "\n",
    "## How Does In-Context Learning Work?\n",
    "\n",
    "- **Prompt as Context:** The user supplies a prompt containing one or more input-output examples (also called “shots”) and a new query.\n",
    "- **Pattern Recognition:** The LLM generalizes from the examples to predict the output for the new query, following the demonstrated pattern or behavior.\n",
    "- **No Training Required:** The model’s weights are not updated; all “learning” happens dynamically from the prompt context.\n",
    "\n",
    "### Example: Few-Shot In-Context Learning\n",
    "\n",
    "```\n",
    "Translate English to French:\n",
    "English: cat -> French: chat\n",
    "English: dog -> French: chien\n",
    "English: bird -> French:\n",
    "```\n",
    "\n",
    "**Model Output:** oiseau\n",
    "\n",
    "### Types of In-Context Learning\n",
    "\n",
    "- **Zero-Shot:** No examples, only instructions (e.g., “Translate to French: dog”)\n",
    "- **One-Shot:** A single example is given in the prompt.\n",
    "- **Few-Shot:** Multiple examples are provided (usually 2–10).\n",
    "\n",
    "### Why is In-Context Learning Important?\n",
    "\n",
    "- **Flexibility:** Allows LLMs to adapt to new tasks and user needs without retraining.\n",
    "- **Rapid Prototyping:** Enables users to experiment and iterate on new tasks via prompt engineering.\n",
    "- **Personalization:** Users can tailor prompts for domain-specific or personalized behaviors.\n",
    "\n",
    "### In-Context Learning vs. Fine-Tuning\n",
    "\n",
    "| In-Context Learning           | Fine-Tuning                      |\n",
    "|------------------------------|----------------------------------|\n",
    "| No model parameter updates   | Model weights updated            |\n",
    "| Fast, dynamic, on-the-fly    | Offline, requires training       |\n",
    "| Adapts to new tasks quickly  | Good for large, static datasets  |\n",
    "| Prompt = “memory”            | Model “remembers” after training |\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Language translation\n",
    "- Text classification\n",
    "- Summarization\n",
    "- Code generation\n",
    "- Custom workflow automation\n",
    "  \n",
    "### References\n",
    "\n",
    "- [OpenAI GPT-3 Paper (Section 3: In-Context Learning)](https://arxiv.org/abs/2005.14165)\n",
    "- [Stanford CS25: In-Context Learning](https://web.stanford.edu/class/cs25/)\n",
    "- [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "\n",
    "**Summary:**  \n",
    "In-Context Learning allows language models to rapidly adapt to new tasks and patterns by observing examples in the prompt, enabling flexible and dynamic application of LLMs without retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f7e6e4-513e-4d5e-90b7-0ca0747c7e81",
   "metadata": {},
   "source": [
    "# Understanding RAG\n",
    "• Indexing <br>\n",
    "• Retrieval <br>\n",
    "• Augmentation <br>\n",
    "• Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ac7fa-3cf5-4fd5-b24d-d8a36410db02",
   "metadata": {},
   "source": [
    "## Indexing - Indexing\n",
    "\n",
    "**Indexing** is the process of **preparing your knowledge base** so that it can be **efficiently searched** at query time.  \n",
    "This step consists of 4 sub-steps.\n",
    "\n",
    "## 1. Document Ingestion\n",
    "- You load your source knowledge into memory\n",
    "- Examples:\n",
    "  - PDF reports, Word documents\n",
    "  - YouTube transcripts, blog pages\n",
    "  - GitHub repos, internal wikis\n",
    "  - SQL records, scraped webpages\n",
    "\n",
    "#### Tools:\n",
    "- LangChain loaders (e.g., `PyPDFLoader`, `YoutubeLoader`, `WebBaseLoader`, `GitLoader`, etc.)\n",
    "\n",
    "#### Illustration:\n",
    "An icon of a web page representing document loader → yellow sheet of paper (Shv)  \n",
    "**LLM → Context**  \n",
    "\n",
    "This process converts raw documents into structured content that can be efficiently retrieved.\n",
    "\n",
    "## 2. Text Chunking\n",
    "- Break large documents into small, semantically meaningful **chunks**\n",
    "\n",
    "#### Why chunk?\n",
    "- LLMs have context limits (e.g., 4K-32K tokens)\n",
    "- Smaller chunks are more focused → better semantic search\n",
    "\n",
    "#### Tools:\n",
    "- RecursiveCharacterTextSplitter\n",
    "- MarkdownHeaderTextSplitter\n",
    "- SemanticChunker\n",
    "\n",
    "##### Illustration:\n",
    "An icon of a web page representing document loader → yellow sheet of paper (chunked text)  \n",
    "**Source** indicates raw document input, and the chunks are the resulting smaller segments for easier processing and retrieval.\n",
    "\n",
    "## 3. Embedding Generation\n",
    "- Convert each chunk into a **dense vector** (embedding) that captures its **meaning**\n",
    "\n",
    "### Why embeddings?\n",
    "- Similar ideas land close together in vector space\n",
    "- Allows fast, fuzzy semantic search\n",
    "\n",
    "### Tools:\n",
    "- OpenAIEmbeddings\n",
    "- SentenceTransformerEmbeddings\n",
    "- InstructorEmbeddings\n",
    "- etc.\n",
    "\n",
    "### Illustration:\n",
    "An icon of a web page representing document loader → yellow sheet of paper (embedded vector)  \n",
    "**Source** indicates raw document, which is then transformed into dense vectors for semantic understanding and retrieval.\n",
    "\n",
    "## 4. Storage in a Vector Store\n",
    "- Store the vectors along with the original chunk text + metadata in a **vector database**\n",
    "\n",
    "### Vector DB options:\n",
    "- **Local**: FAISS, Chroma\n",
    "- **Cloud**: Pinecone, Weaviate, Milvus, Qdrant\n",
    "\n",
    "#### Illustration:\n",
    "Icons representing different vector database options, indicating storage and retrieval.  \n",
    "**Source** shows the storage of vector representations for efficient semantic search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d204566-4a03-4c0b-af7c-14d805c125e8",
   "metadata": {},
   "source": [
    "#### Retrieval\n",
    "- **Retrieval** is the *real-time* process of **finding the most relevant pieces of information** from a **pre-built index** (created during indexing) based on the user's question.\n",
    "\n",
    "#### It’s like asking:\n",
    "> \"From all the knowledge I have, which 3–5 chunks are most helpful to answer this query?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de89d1e7-e8c9-4b03-9d99-89280cfa5ee1",
   "metadata": {},
   "source": [
    "#### Augmentation\n",
    "- **Augmentation** refers to the step where the **retrieved documents** (chunks of relevant context) are **combined with the user’s query** to form a new, enriched prompt for the LLM.\n",
    "\n",
    "#### Generation\n",
    "- **Generation** is the final step where a **Large Language Model (LLM)** uses the **user’s query** and the **retrieved & augmented context** to generate a response."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
