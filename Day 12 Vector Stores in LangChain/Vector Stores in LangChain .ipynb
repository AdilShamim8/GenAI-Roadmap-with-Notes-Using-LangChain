{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bae2111-86bd-4c58-8f44-71851aef7e83",
   "metadata": {},
   "source": [
    "# Vector Stores in LangChain\n",
    "\n",
    "* A **vector store** is a system designed to store and retrieve data represented as **numerical vectors**.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **Storage** – Ensures that vectors and their associated metadata are retained, whether in-memory for quick lookups or on-disk for durability and large-scale use.\n",
    "2. **Similarity Search** – Helps retrieve the vectors most similar to a query vector.\n",
    "3. **Indexing** – Provide a data structure or method that enables fast similarity searches on high-dimensional vectors (e.g., approximate nearest neighbor lookups).\n",
    "4. **CRUD Operations** – Manage the lifecycle of data—adding new vectors, reading them, updating existing entries, removing outdated vectors.\n",
    "\n",
    "### Use-cases\n",
    "\n",
    "1. Semantic Search\n",
    "2. RAG\n",
    "3. Recommender Systems\n",
    "4. Image/Multimedia Search\n",
    "### What are Vector Stores?\n",
    "\n",
    "A **vector store** is a database or storage system built to store and retrieve high-dimensional vectors (embeddings) efficiently. In LangChain and other LLM frameworks, vector stores are used to:\n",
    "\n",
    "- **Store Embeddings:** Save the vector representations (embeddings) of documents, text chunks, or other data.\n",
    "- **Metadata Association:** Store metadata (source, author, etc.) alongside each embedding for richer search/filtering.\n",
    "- **Similarity Search:** At query time, convert the user’s query into an embedding and quickly find stored vectors that are most similar (“closest”) to it.\n",
    "\n",
    "### Why Vector Stores?\n",
    "\n",
    "Vector stores are essential in modern AI and LLM (Large Language Model) applications because they enable fast, scalable, and accurate semantic search and retrieval of information. Here’s why they matter:\n",
    "\n",
    "- **Semantic Search:** Unlike traditional keyword search, vector stores retrieve data based on meaning and context using high-dimensional embeddings. This allows you to find the most relevant documents or text passages for a query, even if no keywords overlap.\n",
    "- **Retrieval-Augmented Generation (RAG):** Vector stores are core to RAG pipelines, allowing LLMs to access external knowledge, documents, or datasets at runtime.\n",
    "- **Performance & Scalability:** They are optimized for efficient similarity search on large datasets, making them suitable for enterprise and production workloads.\n",
    "- **Improved LLM Output:** By retrieving relevant context for a user’s question, vector stores help LLMs generate more accurate and contextually relevant responses.\n",
    "- **Versatile Applications:** Used in chatbots, document Q&A, semantic search engines, knowledge management, and more.\n",
    "\n",
    "### How Vector Stores Work in LangChain\n",
    "\n",
    "1. **Embedding Creation:** Text or document chunks are converted into vector embeddings using an embedding model (e.g., OpenAI, HuggingFace, Cohere).\n",
    "2. **Storing Embeddings:** Embeddings and their associated metadata (e.g., original text, source) are stored in a vector store.\n",
    "3. **Similarity Search:** At query time, the query is embedded and compared (via cosine similarity, dot product, etc.) against stored vectors to retrieve the most relevant documents.\n",
    "\n",
    "### Popular Vector Stores Supported by LangChain\n",
    "\n",
    "- **FAISS:** Fast, open-source library for efficient similarity search.\n",
    "- **Chroma:** Lightweight, simple, and fully open-source vector DB.\n",
    "- **Pinecone:** Fully managed, scalable vector database in the cloud.\n",
    "- **Weaviate:** Open-source vector database with hybrid search and knowledge graph features.\n",
    "- **Qdrant:** Open-source, production-ready vector search engine.\n",
    "- **Milvus:** High-performance, enterprise-grade vector database.\n",
    "- **Elasticsearch/OpenSearch:** Supports vector search as a plugin/extension.\n",
    "- **Redis:** Can be used as a vector store with appropriate configuration.\n",
    "\n",
    "#### Example Workflow\n",
    "\n",
    "#### 1. Create Embeddings and Store in a Vector DB\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Example: Create embeddings for a list of texts\n",
    "texts = [\"Hello World\", \"How are you?\", \"LangChain is awesome!\"]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Store in FAISS vector store\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)\n",
    "```\n",
    "\n",
    "#### 2. Perform Similarity Search\n",
    "\n",
    "```python\n",
    "# Query for similar documents\n",
    "results = vectorstore.similarity_search(\"Hello there!\", k=2)\n",
    "for doc in results:\n",
    "    print(doc.page_content)\n",
    "```\n",
    "\n",
    "#### 3. Persist and Reload Vector Stores\n",
    "\n",
    "Some vector stores (e.g., FAISS, Chroma) allow saving to disk and reloading for future queries.\n",
    "\n",
    "```python\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "# Later...\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Vector Stores\n",
    "\n",
    "- Whenever you want to retrieve relevant chunks of text based on semantic similarity.\n",
    "- For building RAG pipelines, document search, chatbots, FAQ bots, or knowledge bases.\n",
    "- When working with large collections of text or documents.\n",
    "\n",
    "### References\n",
    "\n",
    "- [FAISS GitHub](https://github.com/facebookresearch/faiss)\n",
    "- [Chroma Documentation](https://docs.trychroma.com/)\n",
    "- [Pinecone Documentation](https://docs.pinecone.io/)\n",
    "\n",
    "**Summary:**  \n",
    "Vector stores in LangChain are the backbone for efficient, scalable, and accurate semantic search and retrieval workflows, powering advanced LLM applications by connecting queries to the most relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbccd603-e7d4-4ca6-b5c9-2585135e981a",
   "metadata": {},
   "source": [
    "# Vector Store Vs Vector Database\n",
    "\n",
    "### Vector Store\n",
    "\n",
    "- Typically refers to a lightweight library or service that focuses on storing vectors (embeddings) and performing similarity search.\n",
    "- May not include many traditional database features like transactions, rich query languages, or role-based access control.\n",
    "- Ideal for prototyping, smaller-scale applications.\n",
    "- Examples: FAISS (where you store vectors and can query them by similarity, but you handle persistence and scaling separately).\n",
    "\n",
    "### Vector Database\n",
    "\n",
    "- A full-fledged database system designed to store and query vectors.\n",
    "- Offers additional **\"database-like\"** features:\n",
    "  - Distributed architecture for horizontal scaling.\n",
    "  - Durability and persistence (replication, backup/restore).\n",
    "  - Metadata handling (schemas, filters).\n",
    "  - Potential for ACID or near-ACID guarantees.\n",
    "  - Authentication/authorization and more advanced security.\n",
    "- Geared for production environments with significant scaling, large datasets.\n",
    "- Examples: Milvus, Qdrant, Weaviate.\n",
    "\n",
    "A vector database is effectively a vector store with extra database features (e.g., clustering, scaling, security, metadata filtering, and durability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becbee06-59a0-407c-b969-3b940ce59218",
   "metadata": {},
   "source": [
    "# Vector Stores in LangChain\n",
    "\n",
    "\n",
    "- **Supported Stores:** LangChain integrates with multiple vector stores (FAISS, Pinecone, Chroma, Qdrant, Weaviate, etc.), giving you flexibility in scale, features, and deployment.\n",
    "- **Common Interface:** A uniform Vector Store API lets you swap out one backend (e.g., FAISS) for another (e.g., Pinecone) with minimal code changes.\n",
    "- **Metadata Handling:** Most vector stores in LangChain allow you to attach metadata (e.g., timestamps, authors) to each document, enabling filter-based retrieval.\n",
    "\n",
    "#### Example methods\n",
    "- `from_documents(...)` or `from_texts(...)`\n",
    "- `add_documents(...)` or `add_texts(...)`\n",
    "- `similarity_search(query, k=...)`\n",
    "\n",
    "#### Metadata-Based Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a85e6-61d4-4660-b516-147ba8195057",
   "metadata": {},
   "source": [
    "## Chroma Vector Store\n",
    "\n",
    "Chroma is a lightweight, open-source vector database that is especially friendly for local development and small- to medium-scale production needs.\n",
    "\n",
    "#### Chroma Tenancy and DB Hierarchy\n",
    "\n",
    "- At the top is the **Tenant**.\n",
    "- Each tenant has multiple **Databases**.\n",
    "- Each database contains multiple **Collections**.\n",
    "- Each collection contains multiple **Docs**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
