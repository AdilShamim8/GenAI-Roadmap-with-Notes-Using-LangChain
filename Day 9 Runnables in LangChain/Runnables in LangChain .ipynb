{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225c60ee-4ae3-40cb-8da0-aa8216387d73",
   "metadata": {},
   "source": [
    "# Runnables in LangChain\n",
    "\n",
    "## What are Runnables?\n",
    "\n",
    "**Runnables** are a central abstraction in LangChain that represent any component or chain that can be “run” (executed) to process input and produce output. This includes language models, chains, tools, prompts, and even custom logic. The Runnable interface provides a unified way to handle execution, composition, streaming, and asynchronous runs across all types of LangChain components.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Unified Interface:** All major LangChain components (LLMs, chains, retrievers, tools, agents, etc.) implement the Runnable interface.\n",
    "- **Composability:** Runnables can be chained, branched, or combined using operators like `|` (pipe), `&` (parallel), and more.\n",
    "- **Flexible Execution:** Supports synchronous, asynchronous, and streaming runs.\n",
    "- **Type Safety:** Interfaces are typed, leading to better reliability in pipelines.\n",
    "\n",
    "### Why Use Runnables?\n",
    "\n",
    "- **Consistency:** One interface for running, composing, and managing all pipeline components.\n",
    "- **Modularity:** Easily mix, match, and reuse components in complex workflows.\n",
    "- **Rich Composition:** Enables advanced patterns like branching, parallelism, and conditional execution with simple syntax.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Simple runnable from a lambda (custom function)\n",
    "def to_upper(text):\n",
    "    return text.upper()\n",
    "upper_runnable = RunnableLambda(to_upper)\n",
    "\n",
    "print(upper_runnable.invoke(\"hello\"))  # Output: \"HELLO\"\n",
    "\n",
    "# Composing runnables using | (pipe)\n",
    "llm = OpenAI()\n",
    "prompt = lambda name: f\"Tell me a joke about {name}.\"\n",
    "chain = RunnableLambda(prompt) | llm\n",
    "result = chain.invoke(\"cats\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### Composition Operators\n",
    "\n",
    "- `|` : Pipe output of one runnable to the next (sequential composition)\n",
    "- `&` : Run multiple runnables in parallel (parallel composition)\n",
    "- **Branching, conditional, and mapping**: Advanced controls for dynamic workflows\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- [LangChain Docs: Runnables](https://python.langchain.com/docs/expression_language/)\n",
    "- [LangChain Cookbook: Composition](https://python.langchain.com/docs/cookbook/composition)\n",
    "\n",
    "**Summary:**  \n",
    "Runnables are the building blocks of LangChain workflows, providing a unified, modular, and composable interface for everything you can “run”—from LLMs and prompts to chains, tools, and custom logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c570e7-e83c-4d9e-a518-f048f96f0955",
   "metadata": {},
   "source": [
    "| Chain Name               | Description                                                                     |\n",
    "|--------------------------|---------------------------------------------------------------------------------|\n",
    "| LLMChain                | Basic chain that calls an LLM with a prompt template.                         |\n",
    "| SequentialChain         | Chains multiple LLM calls in a specific sequence.                              |\n",
    "| SimpleSequentialChain   | A simplified version of SequentialChain for easier use.                        |\n",
    "| ConversationalRetrievalChain | Handles conversational Q&A with memory and retrieval.                      |\n",
    "| RetrievalQA             | Retrieves relevant documents and uses an LLM for question-answering.         |\n",
    "| RouterChain             | Directs user queries to different chains based on intent.                     |\n",
    "| MultiPromptChain        | Uses different prompts for different user intents dynamically.                |\n",
    "| HydeChain (Hypothetical Document Embeddings) | Generates hypothetical answers to improve document retrieval.  |\n",
    "| AgentExecutorChain      | Orchestrates different tools and actions dynamically using an agent.        |\n",
    "| SQLDatabaseChain        | Connects to SQL databases and answers natural language queries.              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81aa10e2-914f-45ea-b164-b335be4f0c61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.axis('off')\n",
    "\n",
    "# Function to draw a rectangle with centered text\n",
    "def draw_box(x, y, text, color):\n",
    "    width = 3\n",
    "    height = 1\n",
    "    rect = plt.Rectangle((x, y), width, height, fc=color, ec='black', zorder=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + width/2, y + height/2, text, ha='center', va='center', fontsize=10, zorder=3)\n",
    "\n",
    "# Draw the top node\n",
    "draw_box(3.5, 4.8, 'Runnables', 'yellow')\n",
    "\n",
    "# Draw the child nodes\n",
    "draw_box(1, 2.5, 'Task Specific Runnables', 'lightgreen')\n",
    "draw_box(6, 2.5, 'Runnable Primitives', 'lightgreen')\n",
    "\n",
    "# Draw arrows from top to children\n",
    "ax.annotate('', xy=(2.5, 4.8), xytext=(2.5, 2.5 + 1), arrowprops=dict(arrowstyle='->', linewidth=1.5))\n",
    "ax.annotate('', xy=(8.5, 4.8), xytext=(8.5, 2.5 + 1), arrowprops=dict(arrowstyle='->', linewidth=1.5))\n",
    "\n",
    "# Set limits\n",
    "ax.set_xlim(0, 12)\n",
    "ax.set_ylim(0, 6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e366d71c-b146-4948-9216-06535aa1181d",
   "metadata": {},
   "source": [
    "#### **Task Specific Runnables**\n",
    "- **Definition:** These are *core* LangChain components that have been converted into Runnables so they can be used in pipelines.\n",
    "\n",
    "- **Purpose:** Perform task-specific operations like LLM calls, prompting, retrieval, etc.\n",
    "\n",
    "- **Examples:**\n",
    "  - **ChatOpenAI** — Runs an LLM model.\n",
    "  - **PromptTemplate** — Formats prompts dynamically.\n",
    "  - **Retriever** — Retrieves relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c25e5e-d54c-4c4d-bda3-3f649e30a236",
   "metadata": {},
   "source": [
    "#### **Runnable Primitives**\n",
    "- **Definition:** These are *fundamental* building blocks for structuring execution logic in AI workflows.\n",
    "\n",
    "- **Purpose:** They help orchestrate execution by defining how different Runnables interact (sequentially, in parallel, conditionally, etc.).\n",
    "\n",
    "- **Examples:**\n",
    "  - **RunnableSequence** — Runs steps in order (`[operator]`).\n",
    "  - **RunnableParallel** — Runs multiple steps simultaneously.\n",
    "  - **RunnableMap** — Maps the same input across multiple functions.\n",
    "  - **RunnableBranch** — Implements conditional execution (`if-else` logic).\n",
    "  - **RunnableLambda** — Wraps custom Python functions into Runnables.\n",
    "  - **RunnablePassThrough** — Just forwards input as output (acts as a placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8b855-e9ba-4e43-ad75-1bdca2af5369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d82868e-c9b9-43b6-841c-7d82fb7bccea",
   "metadata": {},
   "source": [
    "## 1. RunnableSequence\n",
    "\n",
    "\n",
    "**RunnableSequence** is a sequential chain of runnables in LangChain that executes each step one after another, passing the output of one step as the input to the next.\n",
    "\n",
    "It is useful when you need to compose multiple runnables together in a structured workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1722b7-a784-4933-b0be-8bed1b2600c7",
   "metadata": {},
   "source": [
    "### What is RunnableSequence?\n",
    "\n",
    "**RunnableSequence** is a core class in LangChain’s [Expression Language](https://python.langchain.com/docs/expression_language/) that allows you to compose multiple Runnables (language models, chains, retrievers, tools, or custom logic) into a pipeline where each step’s output is passed as input to the next step—much like a sequential chain, but with a more flexible and composable interface.\n",
    "\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Sequential Composition:** Connects multiple Runnables into a single pipeline.\n",
    "- **Type Safety:** Maintains input/output types between steps.\n",
    "- **Modular & Reusable:** Each step can be a prompt, LLM, parser, retriever, or another chain.\n",
    "- **Advanced Control:** Supports synchronous, asynchronous, and streaming executions.\n",
    "\n",
    "\n",
    "### Why Use RunnableSequence?\n",
    "\n",
    "- To build complex, multi-step pipelines with clear, readable code.\n",
    "- To mix and match different types of components (prompts, models, output parsers, etc.).\n",
    "- For more maintainable and scalable workflows than hand-rolled sequential logic.\n",
    "\n",
    "\n",
    "### Basic Example\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# Define the steps\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "llm = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Compose them into a RunnableSequence\n",
    "chain = RunnableSequence([prompt, llm, parser])\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"cats\"})\n",
    "print(result)\n",
    "```\n",
    "\n",
    "Or, using pipe (`|`) syntax:\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | parser\n",
    "result = chain.invoke({\"topic\": \"cats\"})\n",
    "```\n",
    "\n",
    "### When to Use RunnableSequence\n",
    "\n",
    "- When you want to combine multiple processing steps into one reusable component.\n",
    "- When you need to enforce a strict flow of data between steps.\n",
    "- For readable, maintainable, and scalable multi-step pipelines.\n",
    "  \n",
    "**Summary:**  \n",
    "`RunnableSequence` lets you build flexible, sequential workflows by connecting any number of Runnables—making complex LLM-powered pipelines easy to construct and maintain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea282e-d1fd-4b52-9c63-abb2e532b612",
   "metadata": {},
   "source": [
    "## 2. RunnableParallel\n",
    "\n",
    "\n",
    "**RunnableParallel** is a runnable primitive that allows multiple runnables to execute in parallel.\n",
    "\n",
    "Each runnable receives the same input and processes it independently, producing a dictionary of outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30cf2e-2340-4bfc-a2a0-0fbe08c12bb1",
   "metadata": {},
   "source": [
    "### What is RunnableParallel?\n",
    "\n",
    "**RunnableParallel** is a class in LangChain’s Expression Language that allows you to run multiple Runnables (language models, chains, tools, or custom logic) in parallel on the same input. Each Runnable processes the input independently, and their results are collected into a single output—usually a dictionary mapping each Runnable’s name to its result.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Parallel Execution:** Multiple Runnables are invoked at the same time, improving efficiency for independent tasks.\n",
    "- **Same Input:** Each Runnable receives the same input.\n",
    "- **Aggregated Output:** Results from all Runnables are returned as a single dictionary.\n",
    "- **Flexible Composition:** Each sub-Runnable can be any chain, tool, model, or custom function.\n",
    "\n",
    "\n",
    "### Why Use RunnableParallel?\n",
    "\n",
    "- To process the same input in multiple, independent ways (e.g., summarization, sentiment analysis, keyword extraction).\n",
    "- To improve performance by avoiding unnecessary sequential steps for unrelated tasks.\n",
    "- For clean, modular, and scalable pipeline design.\n",
    "\n",
    "\n",
    "### Basic Example\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Define sub-runnables\n",
    "prompt_summary = ChatPromptTemplate.from_template(\"Summarize: {text}\")\n",
    "prompt_keywords = ChatPromptTemplate.from_template(\"Extract keywords: {text}\")\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "summary_chain = prompt_summary | llm | parser\n",
    "keywords_chain = prompt_keywords | llm | parser\n",
    "\n",
    "# Compose them in RunnableParallel\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"summary\": summary_chain,\n",
    "    \"keywords\": keywords_chain,\n",
    "})\n",
    "\n",
    "result = parallel_chain.invoke({\"text\": \"LangChain is a framework for building LLM-powered applications.\"})\n",
    "print(result)\n",
    "# Output:\n",
    "# {'summary': 'LangChain is a framework...', 'keywords': 'LangChain, framework, LLM, applications'}\n",
    "```\n",
    "\n",
    "### When to Use RunnableParallel?\n",
    "\n",
    "- When you want independent analysis or transformation of input data in parallel.\n",
    "- When maximizing efficiency for multi-task processing.\n",
    "- For any scenario where you need to aggregate multiple outputs from the same input.\n",
    "\n",
    "**Summary:**  \n",
    "`RunnableParallel` lets you run several processing steps in parallel on the same input and gather their outputs—ideal for efficient, multi-task, and modular LLM pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a146466-5207-4b57-8b99-98759491193d",
   "metadata": {},
   "source": [
    "## 3. RunnablePassthrough\n",
    "\n",
    "\n",
    "**RunnablePassthrough** is a special Runnable primitive that simply returns the input as output without modifying it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9013a2-b875-4e48-8277-5d144a71d270",
   "metadata": {},
   "source": [
    "### What is RunnablePassthrough?\n",
    "\n",
    "**RunnablePassthrough** is a utility class in LangChain’s Expression Language that acts as an identity Runnable—it simply returns its input unchanged. It’s useful when building complex pipelines where you want to pass along input data without modification or when you need to merge original input with additional outputs from other Runnables.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Identity Functionality:** Returns the input as output, unchanged.\n",
    "- **Pipeline Utility:** Useful for merging, branching, or multi-step workflows where unaltered input is needed downstream.\n",
    "- **Composable:** Can be combined with other Runnables for advanced data routing and manipulation.\n",
    "\n",
    "### Why Use RunnablePassthrough?\n",
    "\n",
    "- To preserve and forward the original input alongside other processing results.\n",
    "- When you want to combine the results of multiple Runnables (including the original input) in a pipeline or parallel structure.\n",
    "- For building flexible, reusable, and modular data flows in complex chains.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "passthrough = RunnablePassthrough()\n",
    "\n",
    "# Example: Combine original input and a processed output\n",
    "def to_upper(text):\n",
    "    return text.upper()\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "upper_runnable = RunnableLambda(to_upper)\n",
    "\n",
    "# Run both in parallel, preserving the original\n",
    "parallel = RunnableParallel({\n",
    "    \"original\": passthrough,\n",
    "    \"uppercased\": upper_runnable,\n",
    "})\n",
    "\n",
    "result = parallel.invoke(\"langchain\")\n",
    "print(result)\n",
    "# Output: {'original': 'langchain', 'uppercased': 'LANGCHAIN'}\n",
    "```\n",
    "\n",
    "### When to Use RunnablePassthrough?\n",
    "\n",
    "- When you need to “split” or “fork” the input to keep it available for later steps.\n",
    "- In parallel pipelines where you want to keep the original input along with one or more processed versions.\n",
    "- For debugging, auditing, or enriching outputs with original input context.\n",
    "\n",
    "**Summary:**  \n",
    "`RunnablePassthrough` is a simple but powerful utility for keeping the original input available in complex LangChain pipelines, enabling flexible data routing and rich multi-step workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189c50d0-2870-4ad5-ba23-3da38ed81d2b",
   "metadata": {},
   "source": [
    "## 4. RunnableLambda\n",
    "\n",
    "**RunnableLambda** is a runnable primitive that allows you to apply custom Python functions within an AI pipeline.\n",
    "\n",
    "It acts as a middleware between different AI components, enabling preprocessing, transformation, API calls, filtering, and post-processing in a LangChain workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0497f2-bb6e-4c65-9a9d-16a38e9d7bcc",
   "metadata": {},
   "source": [
    "### What is RunnableLambda?\n",
    "\n",
    "**RunnableLambda** is a utility class in LangChain’s Expression Language that allows you to wrap any Python function (lambda or regular) as a Runnable, making it compatible with LangChain’s pipeline architecture. This means you can use your custom logic—such as data transformations, filtering, or formatting—as a seamless step in your chains, right alongside LLMs, prompts, and other Runnables.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Custom Logic:** Wraps any function for use in LangChain pipelines.\n",
    "- **Composable:** Can be combined with other Runnables using operators like `|` (pipe) and `&` (parallel).\n",
    "- **Flexible Input/Output:** Accepts and returns any type compatible with your pipeline.\n",
    "- **Supports Sync/Async:** Works with both synchronous and asynchronous functions.\n",
    "\n",
    "### Why Use RunnableLambda?\n",
    "\n",
    "- To integrate arbitrary Python logic into your LLM pipelines.\n",
    "- To perform preprocessing, postprocessing, or custom computation.\n",
    "- For transforming or enriching data between LLM and tool steps.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define a function to uppercase text\n",
    "def to_upper(text):\n",
    "    return text.upper()\n",
    "\n",
    "# Wrap it as a Runnable\n",
    "upper_runnable = RunnableLambda(to_upper)\n",
    "\n",
    "print(upper_runnable.invoke(\"langchain\"))  # Output: \"LANGCHAIN\"\n",
    "\n",
    "# Use with LLMs or other Runnables\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "def format_prompt(name):\n",
    "    return f\"Tell me a joke about {name}.\"\n",
    "\n",
    "prompt_runnable = RunnableLambda(format_prompt)\n",
    "llm = OpenAI()\n",
    "\n",
    "chain = prompt_runnable | llm\n",
    "result = chain.invoke(\"cats\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### When to Use RunnableLambda?\n",
    "\n",
    "- To insert custom data transformations or logic in your pipeline.\n",
    "- When you need to preprocess data before sending it to an LLM or tool.\n",
    "- For postprocessing LLM/tool outputs (e.g., cleaning, parsing, formatting).\n",
    "\n",
    "**Summary:**  \n",
    "`RunnableLambda` lets you wrap any Python function as a Runnable, making it a flexible, composable building block in your LangChain workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4f7b9-1062-4736-966d-e73654121791",
   "metadata": {},
   "source": [
    "## 5. RunnableBranch\n",
    "\n",
    "**RunnableBranch** is a control flow component in LangChain that allows you to conditionally route input data to different chains or runnables based on custom logic.\n",
    "\n",
    "It functions like an if/elif/else block for chains — where you define a set of condition functions, each associated with a runnable (e.g., LLM call, prompt chain, or tool). The first matching condition is executed. If no condition matches, a default runnable is used (if provided)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc3d9cd-56dc-45fd-b284-d447be2b09a7",
   "metadata": {},
   "source": [
    "### What is RunnableBranch?\n",
    "\n",
    "**RunnableBranch** is a class in LangChain’s Expression Language that enables dynamic branching in your pipelines. It allows you to direct input to different Runnables (such as LLM chains, tools, or custom functions) based on conditional logic—similar to an if/else or switch/case statement in traditional programming.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Conditional Routing:** Directs input to one of several branches (Runnables) based on a user-defined predicate or function.\n",
    "- **Flexible Logic:** Branching logic can be as simple or as complex as needed, based on input data.\n",
    "- **Composable:** Integrates with other Runnables for modular, readable pipelines.\n",
    "\n",
    "### Why Use RunnableBranch?\n",
    "\n",
    "- To build adaptive workflows that change behavior based on user input, metadata, or intermediate results.\n",
    "- For command interpreters, multi-language bots, or context-aware responses.\n",
    "- To implement decision trees or business logic flows within LLM pipelines.\n",
    "\n",
    "### Example Usage\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "\n",
    "def is_spanish(inputs):\n",
    "    return \"es\" in inputs.get(\"language\", \"\")\n",
    "\n",
    "def is_french(inputs):\n",
    "    return \"fr\" in inputs.get(\"language\", \"\")\n",
    "\n",
    "def default_response(inputs):\n",
    "    return f\"Language '{inputs.get('language')}' not supported.\"\n",
    "\n",
    "spanish_chain = RunnableLambda(lambda inputs: \"¡Hola!\")\n",
    "french_chain = RunnableLambda(lambda inputs: \"Bonjour!\")\n",
    "default_chain = RunnableLambda(default_response)\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    branches=[\n",
    "        (is_spanish, spanish_chain),\n",
    "        (is_french, french_chain),\n",
    "    ],\n",
    "    default=default_chain,\n",
    ")\n",
    "\n",
    "result = branch.invoke({\"language\": \"fr\"})\n",
    "print(result)  # Output: \"Bonjour!\"\n",
    "```\n",
    "\n",
    "- The input is checked against each condition (predicate). The first one that returns True determines which Runnable is executed.\n",
    "- If none match, the `default` branch is used.\n",
    "\n",
    "### When to Use RunnableBranch?\n",
    "\n",
    "- When your pipeline must adapt to different types of inputs or scenarios.\n",
    "- For routing commands, handling multiple languages, or customizing workflows based on user data.\n",
    "- Whenever you need “if/else” or “switch/case” logic in your LLM pipeline.\n",
    "\n",
    "**Summary:**  \n",
    "`RunnableBranch` lets you add flexible, conditional logic to LangChain pipelines, making your workflows smarter and more adaptive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1945f5f4-8681-45ba-a1af-37b2c5d0c36c",
   "metadata": {},
   "source": [
    "## LCEL (LangChain Expression Language)\n",
    "\n",
    "### What is LCEL?\n",
    "\n",
    "**LCEL** stands for **LangChain Expression Language**. It is a flexible and composable abstraction in [LangChain](https://python.langchain.com/docs/expression_language/) that lets you build, compose, and orchestrate LLM workflows using simple, readable Python syntax. LCEL makes it easy to create complex pipelines by connecting different components (like prompts, LLMs, parsers, retrievers, tools, and custom logic) as \"Runnables\".\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Runnables:** The core building blocks—anything that can process input and produce output, such as LLMs, chains, or functions.\n",
    "- **Composition Operators:** Seamlessly combine Runnables using operators:\n",
    "  - `|` (pipe) for sequential composition.\n",
    "  - `&` (ampersand) for parallel execution.\n",
    "- **Branching & Conditional Logic:** Use special Runnables to introduce if/else logic, parallelism, and more.\n",
    "\n",
    "### Why Use LCEL?\n",
    "\n",
    "- **Declarative Pipelines:** Build workflows as readable, modular graphs.\n",
    "- **Reusability:** Easily reuse and share pipeline components.\n",
    "- **Flexibility:** Mix and match prompts, models, tools, and logic.\n",
    "- **Type Safety and Introspection:** Understand input/output types and structure for better debugging and reliability.\n",
    "\n",
    "### Example: Building a Simple LCEL Pipeline\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "\n",
    "# Define steps\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "llm = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Compose a pipeline (RunnableSequence)\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\"topic\": \"cats\"})\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### Advanced: Parallel and Branching\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableParallel, RunnableBranch, RunnableLambda\n",
    "\n",
    "# Parallel: run multiple chains at once\n",
    "parallel = RunnableParallel({\n",
    "    \"joke\": chain,\n",
    "    \"uppercased_topic\": RunnableLambda(lambda x: x[\"topic\"].upper()),\n",
    "})\n",
    "result = parallel.invoke({\"topic\": \"dogs\"})\n",
    "print(result)\n",
    "\n",
    "# Branch: select chain based on input\n",
    "def is_cat(inputs): return \"cat\" in inputs[\"topic\"].lower()\n",
    "def is_dog(inputs): return \"dog\" in inputs[\"topic\"].lower()\n",
    "\n",
    "cat_chain = RunnableLambda(lambda x: \"Meow joke\")\n",
    "dog_chain = RunnableLambda(lambda x: \"Woof joke\")\n",
    "default_chain = RunnableLambda(lambda x: \"Default joke\")\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    branches=[(is_cat, cat_chain), (is_dog, dog_chain)],\n",
    "    default=default_chain\n",
    ")\n",
    "print(branch.invoke({\"topic\": \"cats\"}))  # Output: \"Meow joke\"\n",
    "```\n",
    "\n",
    "### LCEL Building Blocks\n",
    "\n",
    "- **RunnableSequence:** For sequential chains.\n",
    "- **RunnableParallel:** For parallel execution.\n",
    "- **RunnableBranch:** For conditional/branching logic.\n",
    "- **RunnableLambda:** To wrap custom Python functions.\n",
    "- **RunnablePassthrough:** For identity (unmodified) flow.\n",
    "\n",
    "**Summary:**  \n",
    "LCEL is LangChain’s powerful, Pythonic “expression language” for composing modular, reusable, and complex LLM workflows using interoperable Runnables and simple operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a50345-263f-4aa5-9df8-3bf9b612d97c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
